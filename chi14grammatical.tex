\documentclass{sigchi}
\usepackage{subfigure, graphicx, url}

\newcommand{\strong}[1] {\textbf{#1}}
\newcommand{\code}[1] {\texttt{#1}}

\begin{document}
\title{Clausal Complement? Passive Subject?\\Grammatical Relationships for (non) Linguists}
\numberofauthors{2}

\author{% 1st. author
\alignauthor First Lastname\\
% 2nd. author
\alignauthor {Firstname Lastname}\\
}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

What does $X$ do? How is $Y$ described? Most analysts and researchers ask themselves these questions at one point or another while trying to understand a subject.  So far, the field of information retrieval has tackled this problem indirectly. The analyst enters some kind of query, and the system returns some results. When the underlying data has structure, queries can be specific and targeted -- and there is now a substantial body of work [cite] on how best to issue structured queries over \emph{structured} data. But what if the ``data" is is text -- medical literature, legal records, interview transcripts? Although text is richly structured by rules of  grammar and narrative, it is rarely treated as such. While there is a lot of work on extracting various kinds of structured information from text, there is little on how to make it easy for users to access and query over it.  As the questions above show, however, structured queries over language can be extremely useful. In grammatical terms, those two questions become ``What are the verbs of which $X$ is the subject?'' ``What are the adjectives that modify $Y$?''  

Adapting existing structured-query interfaces to grammatical search is problematic for several reasons. The first is that the structures in language are not explicit, like columns in a table, but are implicit, and have to be extracted computationally. Only in the last decade have computational linguistic technologies become fast and accurate enough to use in the real world. 

The second problem is the lack of programming experience among searchers. Current structured query like SPARQL have complex syntaxes that require time and effort to learn. For example, here is a SPARQL query for ``What are all the country capitals in Africa?'':
\begin{verbatim}
SELECT ?capital ?country
WHERE {
  ?x cityname ?capital ;
     isCapitalOf ?y .
  ?y countryname ?country ;
     isInContinent Africa .
}
\end{verbatim}
Even if we assume that only professional analysts engaging in information-intensive work would want to issue such targeted queries,  programming experience is scarce. One study on a query language for  grammatical structures from sentences found that that only 50\% of the people who wanted to use it had programming experience \cite{}.

The third problem is that there are no common-language terms for grammatical relationships even though ordinary people are perfectly capable of understanding and using them. Modern parsers use standard linguistics terminology to label their outputs, but those technical names and definitions are not always accessible to those outside the field. Take the phrases ``he threw the ball" and ``the ball was thrown by him". In both cases, it is clear that `he' is is the one who `threw' whereas, in grammatical terms, the first phrase is in the active case, and the phrase is in the passive case. The Stanford Dependency Parser \cite{}, for example, outputs two different variants of the verb-subject relation: \code{nsubj(he, threw)} and \code{nsubjpass(he, threw)}. A grammatical search system therefore has to bridge the gap between the relations that are recognizable to people and the relations that are extracted from the data.


We conducted experiments to investigate how grammatical relationships between words can be made more recognizable to ordinary people. Following the principle of recognition over recall, we hypothesized that examples would help people identify grammatical relationships more accurately rather than formal terms. We chose Amazon's Mechanical Turk crowdsourcing platform as a source of study participants in order to avoid having any specific backgrounds overrepresented.

We tested two types of examples: a list of matching words, and a list of matching phrases containing the relationship. This is because a grammatical relationship has three components: the two words that enter into the relationship, and the relationship itself. The words are explicitly visible in the text, but the grammatical relationship is implicitly determined by contextual information such as the part of speech of the verb, the relative ordering, and any accompanying words.

We presented participants with a series of identification tasks. In each task, there was a `query' word and a relationship. The participants were shown list of sentences containing that relationship between the query word and other words. Their task was to identify the relationship from list of four choices. We presented the choices in three different ways -- as a short label using linguistic terminology, a linguistic label accompanied by a list of example words that matched, and a linguistic label accompanied by a list of phrases in which that relationship surfaced. 

Our hypothesis was the following:
\begin{quote}
	H1. Grammatical relations can be made more recognizable by showing examples of words or phrases that match.
\end{quote}

Our results confirm that showing examples significantly improves the accuracy with which grammatical relationships are recognized.

In a follow-up study, we found that words seem to be more helpful for relationships involving closed-class words or typical words, but that in all other situations, phrases are more helpful.




\bibliographystyle{acm-sigchi}
\bibliography{papers}

\end{document}


