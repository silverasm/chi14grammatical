\documentclass{sig-alternate}
\usepackage{subfigure, graphicx, url}

\newcommand{\strong}[1] {\textbf{#1}}
\newcommand{\code}[1] {\texttt{#1}}

\begin{document}
\title{Clausal Complement? Passive Subject?\\Grammatical Relationships for (non) Linguists}
\numberofauthors{2}

\author{% 1st. author
\alignauthor First Lastname\\
% 2nd. author
\alignauthor {Firstname Lastname}\\
}

\maketitle

\begin{abstract}
What does $X$ do? How is $Y$ described? These are questions that most analyst ask themselves at one point or another while trying to understand a subject. So far, the field of information retrieval has tackled this problem indirectly. A searcher enters some kind of query, and the system returns some results. When the underlying data has structure, queries can be specific and targeted -- and there is now a substantial body of work [cite] on how best to issue\emph{structured} queries over \emph{structured} data. But what if the  ``data" is is text -- newspaper articles, interview transcripts, books? Although text \emph{is} richly structured by rules of syntax, grammar and narrative, it is rarely treated as such, and there is very little work [cite broccoli] on issuing structured queries over it. Structured queries over grammar, however, can be extremely useful. In grammatical terms, the questions above become precise: What are the verbs of which $X$ is the subject? What are the adjectives that modify $Y$? 

The main difficulty is that the `structures' in language are not explicit, like columns in a table, but are implicit, and have to be extracted computationally.  Only in the last decade have computational linguistic technologies become fast and accurate enough to use in the real world. However, now that syntactic parsers (that calculate the phrase structure of sentences) and dependency parsers (that relationships between words) are fast and fairly accurate, we are revisiting this question.


\end{abstract}

\bibliographystyle{abbrv}
\bibliography{papers}

\end{document}


